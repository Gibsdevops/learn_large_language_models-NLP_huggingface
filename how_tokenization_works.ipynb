{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT7X2ky4J24+wfSBBDVd5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibsdevops/learn_large_language_models-NLP_huggingface/blob/main/how_tokenization_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first type of tokenizer that comes to mind is word-based. It’s generally very easy to set up and use with only a few rules, and it often yields decent results."
      ],
      "metadata": {
        "id": "nRlFd-maqrHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYDvJccFqh2g"
      },
      "outputs": [],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and saving\n",
        "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: from_pretrained() and save_pretrained(). These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
      ],
      "metadata": {
        "id": "RnURMl9HtSJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "081m_Mvhqti5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
      ],
      "metadata": {
        "id": "kIAVOIn6t3wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "hvVes62Nti_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "id": "3vlSk248t8sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a tokenizer is identical to saving a model:"
      ],
      "metadata": {
        "id": "0ZMbJVgnuCRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "IdRXFd1DuAH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n",
        "\n",
        "The tokenization process is done by the tokenize() method of the tokenizer:"
      ],
      "metadata": {
        "id": "yaSbmt_Au374"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaJrzISRu3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "ER5TlLOwuEIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with transformer, which is split into two tokens: transform and ##er.\n",
        "\n",
        "From tokens to input IDs\n",
        "\n",
        "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
      ],
      "metadata": {
        "id": "Uba5eaeNvESB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "ETbcPql9u8aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_NadGi6vIcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}