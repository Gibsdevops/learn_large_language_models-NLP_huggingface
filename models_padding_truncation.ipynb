{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1hZihDCN7Q9GQxb0SbJR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibsdevops/learn_large_language_models-NLP_huggingface/blob/main/models_padding_truncation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9reFUvs-e1a_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "encoded_input = tokenizer(\"hello, I'm a single sentence\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can decode the input IDs to get back the original text:\n",
        "\n",
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "6hClACYkf5nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hence u can encode multiple tokens at once\n",
        "\n",
        "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "Pp9pxtyngf8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if u a want u can ask the tokenozer to return tensors\n",
        "\n",
        "encoded_input_2 = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")\n",
        "print(encoded_input_2)"
      ],
      "metadata": {
        "id": "dLlIgxGzg-7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding inputs\n",
        "\n",
        "If we ask the tokenizer to pad the inputs, it will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one:\n",
        "\n"
      ],
      "metadata": {
        "id": "zrcRLOFejWyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "0vYx1tTPhOvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truncating inputs\n",
        "\n",
        "The tensors might get too big to be processed by the model. For instance, BERT was only pretrained with sequences up to 512 tokens, so it cannot process longer sequences. If you have sequences longer than the model can handle, you’ll need to truncate them with the truncation parameter:"
      ],
      "metadata": {
        "id": "dYKSzGZCjkX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
        "    truncation=True,\n",
        ")\n",
        "print(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "DBtYOiK7jbCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=5,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "X9VHTocKjmwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding special tokens\n",
        "\n",
        "Special tokens (or at least the concept of them) is particularly important to BERT and derived models. These tokens are added to better represent the sentence boundaries, such as the beginning of a sentence ([CLS]) or separator between sentences ([SEP]). Let’s look at a simple example:"
      ],
      "metadata": {
        "id": "tADz3ieYjuXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"How are you?\")\n",
        "print(encoded_input[\"input_ids\"])\n",
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "p_2yj75-jqEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViskJ_lbjwYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}